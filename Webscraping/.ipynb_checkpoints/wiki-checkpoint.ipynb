{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59eea165-b5d7-4b2f-b1e0-41bcbd652025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d6b5768-3b38-48e6-83ec-c5b798268466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_wikipedia_page(url):\n",
    "    \"\"\"Scrapes a Wikipedia page given its URL.\"\"\"\n",
    "    response = req.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to load page: {response.status_code}\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    return {\n",
    "        \"title\": get_article_titles(soup),\n",
    "        \"text_by_heading\": get_article_text(soup),\n",
    "        \"wikipedia_links\": extract_wikipedia_links(soup)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d38cc73-5004-4730-ae34-ee4f931e7510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_titles(soup):\n",
    "#     \"\"\"Finds all the article titles in the HTML page.\"\"\"\n",
    "     return [title.text.strip() for title in soup.find_all('h2')]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05848d30-d495-4962-86dd-6f20f5ac7b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_text(soup):\n",
    "#     \"\"\"Extracts article text for each paragraph with their respective headings.\"\"\"\n",
    "     content = {}\n",
    "     main_content = soup.find('div', {'id': 'bodyContent'})\n",
    "\n",
    "     if main_content:\n",
    "         for section in main_content.find_all(['h2', 'h3']):\n",
    "             heading = section.text.strip()\n",
    "             paragraphs = []\n",
    "             for sibling in section.find_next_siblings():\n",
    "                 if sibling.name in ['h2', 'h3']:\n",
    "                     break\n",
    "                 if sibling.name == 'p':\n",
    "                     paragraphs.append(sibling.text.strip())\n",
    "             content[heading] = ' '.join(paragraphs)\n",
    "     return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e84fc98d-db50-4442-9226-61d83afe6096",
   "metadata": {},
   "outputs": [],
   "source": [
    " def extract_wikipedia_links(soup):\n",
    "     \"\"\"Collects all links that redirect to another Wikipedia page.\"\"\"\n",
    "     links = set()\n",
    "     base_url = \"https://en.wikipedia.org\"\n",
    "     for a_tag in soup.find_all('a', href=True):\n",
    "         href = a_tag['href']\n",
    "         if href.startswith('/wiki/') and ':' not in href:  # Exclude special links like files\n",
    "             full_url = urljoin(base_url, href)\n",
    "             links.add(full_url)\n",
    "     return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d3140c8-1b59-4006-9c4a-66b07daf464e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ['Contents', 'Taxonomy and systematics', 'Description', 'Distribution and habitat', 'Behaviour and ecology', 'Status', 'Notes', 'References']\n",
      "\n",
      "Text by Heading:\n",
      "\n",
      "Taxonomy and systematics:\n",
      "...\n",
      "\n",
      "\n",
      "List of species:\n",
      "...\n",
      "\n",
      "\n",
      "Description:\n",
      "...\n",
      "\n",
      "\n",
      "Vocalisations:\n",
      "...\n",
      "\n",
      "\n",
      "Distribution and habitat:\n",
      "...\n",
      "\n",
      "\n",
      "Behaviour and ecology:\n",
      "...\n",
      "\n",
      "\n",
      "Feeding:\n",
      "...\n",
      "\n",
      "\n",
      "Breeding:\n",
      "...\n",
      "\n",
      "\n",
      "Predators and parasites:\n",
      "...\n",
      "\n",
      "\n",
      "Status:\n",
      "...\n",
      "\n",
      "\n",
      "Notes:\n",
      "...\n",
      "\n",
      "\n",
      "References:\n",
      "...\n",
      "\n",
      "\n",
      "Wikipedia Links:\n",
      "['https://en.wikipedia.org/wiki/Coverts', 'https://en.wikipedia.org/wiki/INaturalist', 'https://en.wikipedia.org/wiki/Sandgrouse', 'https://en.wikipedia.org/wiki/British_Ornithologists%27_Union', 'https://en.wikipedia.org/wiki/Lopholaimus', 'https://en.wikipedia.org/wiki/Starnoenas', 'https://en.wikipedia.org/wiki/Phapitreron', 'https://en.wikipedia.org/wiki/Solomon_Islands_(archipelago)', 'https://en.wikipedia.org/wiki/IUCN_Red_List', 'https://en.wikipedia.org/wiki/Sister_group']\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "if __name__ == \"__main__\":\n",
    "     test_url = \"https://en.wikipedia.org/wiki/Mountain_pigeon\"\n",
    "     scraped_data = scrape_wikipedia_page(test_url)\n",
    "\n",
    "     # Print results\n",
    "     print(\"Title:\", scraped_data[\"title\"])\n",
    "     print(\"\\nText by Heading:\")\n",
    "     for heading, text in scraped_data[\"text_by_heading\"].items():\n",
    "         print(f\"\\n{heading}:\\n{text[:200]}...\\n\")\n",
    "\n",
    "     print(\"\\nWikipedia Links:\")\n",
    "     print(list(scraped_data[\"wikipedia_links\"])[:10])  # Display first 10 links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e8d8f4-17e0-42ee-a4f6-7f74b00bea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This script is a simple yet effective way to scrape and organize information from Wikipedia pages.\n",
    "    It demonstrates the use of web scraping techniques in Python,\n",
    "    handling HTTP requests, and parsing HTML content.\"\"\"\n",
    "\"\"\"This code is a Python script that scrapes content from a Wikipedia page using the requests library to fetch the HTML content and BeautifulSoup to parse it.\n",
    "The script extracts article titles, text organized by headings, and links to other Wikipedia pages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
